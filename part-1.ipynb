{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "raw_df = pd.read_csv('data.csv')\n",
    "# raw_df.info() # display all the columns and their data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first thing to do is choosing what features could be useful for training the model. Let's start by discriminating what features definitely won't contribute and then choosing which ones could.\n",
    "\n",
    "As rule of thumb, using unique non-numerical values is not useful since we want the model to generalize. So columns like `ID` and `Name` have to go. Features that are highly correlated to each other or are directly calculations don't need to be included twice. For example, if we store the birth date, we don't need to include the age since it is a result from the prior. In this case, `Overall` is the score resulting from the rest of the features, so there's no need to have it.\n",
    "\n",
    "Knowing what features influence the outcome of the objective one is a complex task. For the sake of this post lenght, we'll arbitrarily pick some based on what we think impacts a player's skill. Once the features are selected, let's make a new data frame with only the colums we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age Preferred Foot Height  Weight  Agility  Strength  Dribbling  Jumping  \\\n",
      "0   31           Left    5'7  159lbs     91.0      59.0       97.0     68.0   \n",
      "1   33          Right    6'2  183lbs     87.0      79.0       88.0     95.0   \n",
      "2   26          Right    5'9  150lbs     96.0      49.0       96.0     61.0   \n",
      "3   27          Right    6'4  168lbs     60.0      64.0       18.0     67.0   \n",
      "4   27          Right   5'11  154lbs     79.0      75.0       86.0     63.0   \n",
      "\n",
      "   Marking  Interceptions Position  \n",
      "0     33.0           22.0       RF  \n",
      "1     28.0           29.0       ST  \n",
      "2     27.0           36.0       LW  \n",
      "3     15.0           30.0       GK  \n",
      "4     68.0           61.0      RCM  \n"
     ]
    }
   ],
   "source": [
    "# it's useful to have the columns in a list you can modify easily\n",
    "useful_columns = [\n",
    "    'Age',\n",
    "    'Preferred Foot',\n",
    "    'Height',\n",
    "    'Weight',\n",
    "    'Agility',\n",
    "    'Strength',\n",
    "    'Dribbling',\n",
    "    'Jumping',\n",
    "    'Marking',\n",
    "    'Interceptions',\n",
    "    'Position', # I like to let my objective columns at the end\n",
    "]\n",
    "working_df = raw_df[useful_columns]\n",
    "print(working_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's face one of the most common problems in cleaning data: null values. A quick way to check if the dataset has some missing values is using de `isna()` function from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                0\n",
       "Preferred Foot    48\n",
       "Height            48\n",
       "Weight            48\n",
       "Agility           48\n",
       "Strength          48\n",
       "Dribbling         48\n",
       "Jumping           48\n",
       "Marking           48\n",
       "Interceptions     48\n",
       "Position          60\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, we have missing values. Since they represent a very low percentage of the overall value dataset, we can just get rid of them using `dropna` with the `inplace=True` parameter in order to apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuik/venv/ml/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "working_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a working data set, let's start processing the data. There are two basic types of transformations we can apply:\n",
    "\n",
    "- **Encoding**: for getting a numerical representation of categorical values (usally strings)\n",
    "- **Scaling**: for normalizing and setting up continious values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "Encoding is one of the easiest things to undesrtand, since at its core it is just as simple as assigning one unique numerical value to each unique categorical value. For example, on the `Position` column of the data set that would mean:\n",
    "\n",
    "- `CB -> 0`\n",
    "- `CM -> 1`\n",
    "- `GK -> 2`\n",
    "- `LB -> 3`\n",
    "- `ST -> 4`\n",
    "- ...\n",
    "\n",
    "We can achieve this with the `LabelEncoder` class in the `preprocessing` module of Scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 26, 14, ..., 26, 24,  4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# create the encoder for the colum\n",
    "position_encoder = LabelEncoder()\n",
    "# learn the classes and assign a code to each\n",
    "position_encoder.fit(working_df['Position'])\n",
    "\n",
    "# get the encoded column\n",
    "encoded_position = position_encoder.transform(working_df['Position'])\n",
    "encoded_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting back the original classes from the numerical representations, we can use the `inverse_transform` method from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RF', 'ST', 'LW', ..., 'ST', 'RW', 'CM'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_encoder.inverse_transform(encoded_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "For continious values, there's no reason to transform them into numerical representations. How ever, a model can have some difficulties on learning patterns in the data if it has a very large range of values.\n",
    "\n",
    "For example, a model could take some time learning patterns for values ranging from `0 to 255` (like the intensity of a pixel in an image), but it'd take a very short time with values from `0 to 1`. Since a value of `125` in a range from `0 to 255` represents the sames as `0.5` in a range from `0 to 1`, we can change the scale of the first range in order to make it more efficient. This is what the **scaling** process does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.525 ],\n",
       "       [0.775 ],\n",
       "       [0.4   ],\n",
       "       ...,\n",
       "       [0.1875],\n",
       "       [0.3875],\n",
       "       [0.5375]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# There are many types of scalers\n",
    "strength_scaler = MinMaxScaler()\n",
    "# Note that the scalers receive a 2D array as input\n",
    "strength_scaler.fit(working_df[['Strength']])\n",
    "# Get the scaled version\n",
    "scaled_strength = strength_scaler.transform(working_df[['Strength']])\n",
    "scaled_strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing the pipeline\n",
    "\n",
    "How to store the transformed values depends on your preference and attributes of your data set. Since this case has a data set relatively small, we can create a copy of it with the transformed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Preferred Foot  Position   Agility  Strength  Dribbling  Jumping   Marking  \\\n",
      "0               0        21  0.939024    0.5250   1.000000   0.6625  0.329670   \n",
      "1               1        26  0.890244    0.7750   0.903226   1.0000  0.274725   \n",
      "2               1        14  1.000000    0.4000   0.989247   0.5750  0.263736   \n",
      "3               1         5  0.560976    0.5875   0.150538   0.6500  0.131868   \n",
      "4               1        19  0.792683    0.7250   0.881720   0.6000  0.714286   \n",
      "\n",
      "   Interceptions  \n",
      "0       0.213483  \n",
      "1       0.292135  \n",
      "2       0.370787  \n",
      "3       0.303371  \n",
      "4       0.651685  \n"
     ]
    }
   ],
   "source": [
    "# New data frame\n",
    "clean_df = pd.DataFrame()\n",
    "\n",
    "# I will create a dictionary for storing all my encoders\n",
    "encoders = {\n",
    "    'Preferred Foot': LabelEncoder(),\n",
    "    'Position': LabelEncoder()\n",
    "}\n",
    "\n",
    "# Encode all the categorical features\n",
    "for col, encoder in encoders.items():\n",
    "    encoder.fit(working_df[col])\n",
    "    clean_df[col] = encoder.transform(working_df[col])\n",
    "\n",
    "scalers = {\n",
    "    'Agility': MinMaxScaler(),\n",
    "    'Strength': MinMaxScaler(),\n",
    "    'Dribbling': MinMaxScaler(),\n",
    "    'Jumping': MinMaxScaler(),\n",
    "    'Marking': MinMaxScaler(),\n",
    "    'Interceptions': MinMaxScaler()\n",
    "}\n",
    "\n",
    "# Scale all the continous features\n",
    "for col, scaler in scalers.items():\n",
    "    scaler.fit(working_df[[col]])\n",
    "    clean_df[col] = scaler.transform(working_df[[col]])\n",
    "\n",
    "print(clean_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new `clean_df` looks really good for now, so I'm gonna finish this part of the series by exporting it into a new file I can keep using later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('clean_data.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
